{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "dan.monica.smith@gmail.com",
   "authorId": "8939921151237",
   "authorName": "SPORTY4992",
   "lastEditTime": 1738172475408,
   "notebookId": "zxwxd3mothe7zshsbwod",
   "sessionId": "b705b03e-71a1-41b1-bbef-c67845cd9846"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c5fb3f-0470-4cb8-bdf9-c4a7afa6231c",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Load data into Snowflake tables\n",
    "To begin, download the full dataset (1 million rows) as a zip from this [Kaggle link](https://www.kaggle.com/datasets/sridharstreaks/insurance-data-for-machine-learning). Then unzip it to a .csv. Load that .csv into your notebook's files directory. We will pull from that csv to create a training data table and our incoming \"streamed\" data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import pandas as pd\n",
    "\n",
    "# Create the session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329d8a5-1094-4e60-a84a-2d3038aa3c0f",
   "metadata": {
    "name": "cell5"
   },
   "source": [
    "Load data from csv into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760cf92b-9386-4492-a830-b6df0b759560",
   "metadata": {
    "name": "cell6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load full 1M dataset into dataframe\n",
    "insurance_df = pd.read_csv('insurance_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66b854-9e69-4c69-b2ce-921fd86c1b49",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "Data cleaning, rearranging columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde22f4-f7f3-4f17-80f3-19e7399811bf",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# Capitalize column names\n",
    "insurance_df.columns = insurance_df.columns.str.upper()\n",
    "\n",
    "# Rearrange columns to fit target schema\n",
    "cols = insurance_df.columns.tolist()\n",
    "cols = cols[:3] + cols[-1:] + cols[3:-1]\n",
    "insurance_df = insurance_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a79e5-76f2-44d7-a26f-da1b32a80235",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "Use the write_pandas() method to write the first 10k rows into the 'SOURCE_OF_TRUTH' table created with the SQL commands in the SQL file. The method \"returns a Snowpark DataFrame object referring to the table where the pandas DataFrame was written to.\" (Snowpark Documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5421d-3e61-44a6-bd61-c91ad7731d1f",
   "metadata": {
    "name": "cell9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "source_of_truth_df = session.write_pandas(insurance_df[:10000], table_name='SOURCE_OF_TRUTH',database='INSURANCE',schema='ML_PIPE',auto_create_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434ba41-9058-4b3b-89af-8bbdb07d1d65",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "The code below writes the remaining 990k to the INCOMING_DATA_SOURCE table to simulate data being streamed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137111d-ef9c-4bad-8e49-4edfbfe3189c",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "incoming_data_source_df = session.write_pandas(insurance_df[10000:], table_name='INCOMING_DATA_SOURCE',database='INSURANCE',schema='ML_PIPE',auto_create_table=True)"
   ]
  }
 ]
}